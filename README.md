# mobile inference of open-weight LLMs with llama.cpp
